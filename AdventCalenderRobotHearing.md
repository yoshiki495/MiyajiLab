# ロボット聴覚オープンソフトウェアの（OSS）の何たるかを説いてみる

## 1. はじめに

はいどうも、アドベントカレンダー4日目を担当する3年の[yoshiki495](https://github.com/yoshiki495)です。

先日、HRIと京大が開発・公開しているHARK（Honda Research Institute Japan Audition for Robots with Kyoto University）というロボット聴覚のオープンソースソフトウェア（OSS）の講習会にお邪魔してきました。

ちなみにこのロボット聴覚という研究分野、日本初だとか。さすが京大と言わざるを得ません。

そんなわけでこの記事ではHARKというものを筆者の知識を織り混ぜながらできるだけわかりやすく説いていこうと思います。

ではどうぞ。

## 2. ロボット聴覚とは

まずロボット聴覚とは何か。

それは「私たちが普段聞いている音環境をロボットがいかに理解できるようにするかという問題を扱う研究分野のこと」です。

音環境を理解させると簡単に言っていますが、実際はかなり挑戦的です。

私たちの耳は様々な音源を聞き中で、特定の音源をどこから来た何の音源なのかを即座に言い当てることができます。

これをいわゆる「カクテルパーティー効果」と言いますが、

ロボット聴覚として理解させるためには、

1. 音源定位（音がどこから来るのか推定する）
2. 音源分離（特定の音を分離する）
3. 音声認識（分離した音を認識する）

の3ステップを忠実に行う必要があります。

## 2. HARK の概要

### 2.1 HARK の基本

HARKの主要機能と処理の流れは以下のようになっています。

![hark_overview](https://user-images.githubusercontent.com/68012132/204194793-9cbfe489-484b-4c65-a1ea-b996c067e3ce.jpeg)、

上記の通り、HARKは音源定位・追跡、音源分離・強調、音声認識の3主要機能をパッケージ化したソフトウェアなのでマイクロホンアレイとアプリケーションは別途必要になります。

### 2.2 HARK の設計思想

### 参考
- https://www.jstage.jst.go.jp/article/itej/71/9/71_647/_pdf
- https://www.slideshare.net/DaichiKitamura/acoustic-modeling-in-audio-source-separation
