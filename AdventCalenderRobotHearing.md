# ロボット聴覚オープンソフトウェアの（OSS）の何たるかを説いてみる

## 1. はじめに

はいどうも、アドベントカレンダー4日目を担当する3年の[yoshiki495](https://github.com/yoshiki495)です。

先日、HRIと京大が開発・公開しているHARK（Honda Research Institute Japan Audition for Robots with Kyoto University）というロボット聴覚のオープンソースソフトウェア（OSS）の講習会にお邪魔してきました。

ちなみにこのロボット聴覚という研究分野、日本初だとか。さすが京大と言わざるを得ません。

そんなわけでこの記事ではHARKというものを筆者の知識を織り混ぜながらできるだけわかりやすく説いていこうと思います。

ではどうぞ。

## 2. ロボット聴覚とは

まずロボット聴覚とは何か。

それは「私たちが普段聞いている音環境をロボットがいかに理解できるようにするかという問題を扱う研究分野のこと」です。

音環境を理解させると簡単に言っていますが、実際はかなり挑戦的です。

私たちの耳は様々な音源を聞き中で、特定の音源をどこから来た何の音源なのかを即座に言い当てることができます。

これをいわゆる「カクテルパーティー効果」と言いますが、

ロボット聴覚はこの「カクテルパーティー効果」を完全再現するのが目的と言って良いでしょう。

## 2. HARK の概要

### 2.1 HARK の基本

では実際にHARKの概要を見てみましょう。

HARKの主要機能と処理の流れは以下のようになっています。

![hark_overview](https://user-images.githubusercontent.com/68012132/204194793-9cbfe489-484b-4c65-a1ea-b996c067e3ce.jpeg)

主要機能をそれぞれ補足していくと、

- **音源定位・追跡**
  - 実時間で音源の到来方向を推定、その音源を追跡する
- **音源分離・強調**
  - 特定の方向から到来する音源を分離抽出する
- **音声認識**
  - 分離した音源を認識する

って感じです。

上記の通り、HARKは音源定位・追跡、音源分離・強調、音声認識の3主要機能をパッケージ化したソフトウェアなのでマイクロホンアレイとアプリケーションは別途必要になります。

![スクリーンショット 2022-12-03 17 58 38](https://user-images.githubusercontent.com/68012132/205433001-1b3bc3ef-2be7-4015-97af-76b99f9d291b.png)

### 2.2 HARK の設計思想

### 参考
- https://www.jstage.jst.go.jp/article/itej/71/9/71_647/_pdf
- https://www.slideshare.net/DaichiKitamura/acoustic-modeling-in-audio-source-separation
